{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSC 180 Intelligent Systems \n",
    "\n",
    "#### William Lorence\n",
    "\n",
    "#### California State University, Sacramento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Modern Low Footprint Cyber Attack Protection\n",
    "## Reading the Data\n",
    "The code below reads the data from the dataset and creates dataframes. Values of \"-\" are treated as N/A and entries with this value are dropped from the dataframe. The \"attack_cat\" column has been dropped to decrease the likelihood of model overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81173\n",
      "35179\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.681642</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>628</td>\n",
       "      <td>770</td>\n",
       "      <td>13.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2.093085</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>62</td>\n",
       "      <td>28</td>\n",
       "      <td>56329</td>\n",
       "      <td>2212</td>\n",
       "      <td>42.520967</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>udp</td>\n",
       "      <td>snmp</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>500000.001300</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.393556</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>860</td>\n",
       "      <td>1096</td>\n",
       "      <td>43.195886</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.338017</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>FIN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>998</td>\n",
       "      <td>268</td>\n",
       "      <td>44.376468</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
       "3    4  1.681642   tcp     ftp   FIN     12     12     628     770   \n",
       "11  12  2.093085   tcp    smtp   FIN     62     28   56329    2212   \n",
       "15  16  0.000002   udp    snmp   INT      2      0     138       0   \n",
       "17  18  0.393556   tcp    http   FIN     10      8     860    1096   \n",
       "21  22  0.338017   tcp    http   FIN     10      6     998     268   \n",
       "\n",
       "             rate  ...  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
       "3       13.677108  ...                 1                 1               3   \n",
       "11      42.520967  ...                 1                 1               2   \n",
       "15  500000.001300  ...                 1                 1               4   \n",
       "17      43.195886  ...                 1                 1               2   \n",
       "21      44.376468  ...                 1                 1               1   \n",
       "\n",
       "    is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  \\\n",
       "3              1           1                 0           2           1   \n",
       "11             0           0                 0           1           1   \n",
       "15             0           0                 0           2           1   \n",
       "17             0           0                 1           1           3   \n",
       "21             0           0                 1           2           3   \n",
       "\n",
       "    is_sm_ips_ports  label  \n",
       "3                 0      0  \n",
       "11                0      0  \n",
       "15                0      0  \n",
       "17                0      0  \n",
       "21                0      0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"./dataset/\"\n",
    "save_path = \"./models/\"\n",
    "\n",
    "#Defines filepaths for the data sets\n",
    "training_set = os.path.join(path,\"UNSW_NB15_training-set.csv\")\n",
    "test_set = os.path.join(path,\"UNSW_NB15_test-set.csv\")\n",
    "\n",
    "#Loads files into dataframes\n",
    "df_training_set = pd.read_csv(training_set, na_values = ['-'])\n",
    "df_test_set = pd.read_csv(test_set, na_values = ['-'])\n",
    "\n",
    "#Removes rows with a \"-\" in any column\n",
    "df_training_set.dropna(inplace = True)\n",
    "df_test_set.dropna(inplace = True)\n",
    "\n",
    "#Drop the \"attack_cat\" column from both the training and test datasets\n",
    "df_training_set.drop(columns=['attack_cat'], inplace = True)\n",
    "df_test_set.drop(columns=['attack_cat'], inplace = True)\n",
    "\n",
    "print(len(df_training_set))\n",
    "print(len(df_test_set))\n",
    "\n",
    "df_training_set.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering\n",
    "The code below removes categorical values that are not present in both datasets. As visible via the print statements, roughly 600 entries are dropped from the training set, while only one is dropped from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udp', 'tcp'}\n",
      "{'udp', 'tcp'}\n",
      "{'udp', 'tcp'}\n",
      "{'radius', 'http', 'smtp', 'dhcp', 'ftp', 'ssh', 'ssl', 'pop3', 'dns', 'ftp-data', 'snmp', 'irc'}\n",
      "{'radius', 'smtp', 'dhcp', 'ftp', 'ssh', 'ssl', 'snmp', 'pop3', 'irc', 'http', 'dns', 'ftp-data'}\n",
      "{'radius', 'http', 'smtp', 'dhcp', 'ftp-data', 'ssh', 'ftp', 'pop3', 'dns', 'ssl', 'snmp', 'irc'}\n",
      "{'FIN', 'INT', 'CON', 'RST', 'REQ'}\n",
      "{'FIN', 'ACC', 'INT', 'CON', 'REQ'}\n",
      "{'INT', 'CON', 'FIN', 'REQ'}\n",
      "81159\n",
      "35178\n"
     ]
    }
   ],
   "source": [
    "#Removes categorical values not present in both datasets\n",
    "categorical_columns = ['proto', 'service', 'state']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    unique_values_training = set(df_training_set[column].unique())\n",
    "    unique_values_test = set(df_test_set[column].unique())\n",
    "\n",
    "    print(unique_values_training)\n",
    "    print(unique_values_test)\n",
    "    \n",
    "    common_values = unique_values_training.intersection(unique_values_test)\n",
    "    print(common_values)\n",
    "\n",
    "    df_training_set = df_training_set[df_training_set[column].isin(common_values)]\n",
    "    df_test_set = df_test_set[df_test_set[column].isin(common_values)]\n",
    "\n",
    "print(len(df_training_set))\n",
    "print(len(df_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'id' and 'is_sm_ips_ports' are dropped from the dataframes: 'id' is irrelevent to the data at hand, and 'is_sm_ips_ports' causes errors when calculating z scores (likely because it is always 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_set.drop('id', axis = 1, inplace = True)\n",
    "df_training_set.drop('is_sm_ips_ports', axis = 1, inplace = True)\n",
    "\n",
    "df_test_set.drop('id', axis = 1, inplace = True)\n",
    "df_test_set.drop('is_sm_ips_ports', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical data is then normalized via z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds numerical data (columns that are not categorical)\n",
    "numerical_columns = set(df_training_set.columns.symmetric_difference(categorical_columns))\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def z_score_numerical(df, names):\n",
    "    for name in names:\n",
    "        df[name] = zscore(df[name])\n",
    "\n",
    "z_score_numerical(df_training_set, numerical_columns)\n",
    "z_score_numerical(df_test_set, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the next bit of code encodes the now-filtered categorical values into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue) given a dataframe and a list of column names\n",
    "def encode_text_dummy_loop(df, names):\n",
    "    for name in names:\n",
    "        dummies = pd.get_dummies(df[name])\n",
    "        for x in dummies.columns:\n",
    "            dummy_name = \"{}-{}\".format(name, x)\n",
    "            df[dummy_name] = dummies[x]\n",
    "        df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy_loop(df_training_set, categorical_columns)\n",
    "encode_text_dummy_loop(df_test_set, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "Split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60869, 56)\n",
      "Test set shape: (35178, 56)\n"
     ]
    }
   ],
   "source": [
    "# Ensure both sets have the same columns after encoding\n",
    "df_training_set, df_test_set = df_training_set.align(df_test_set, join='inner', axis=1)\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X_train = df_training_set.drop(columns=['label'])\n",
    "y_train = df_training_set['label']\n",
    "X_test = df_test_set.drop(columns=['label'])\n",
    "y_test = df_test_set['label']\n",
    "\n",
    "# Split training data for validation\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Check data shapes\n",
    "print(f'Training set shape: {X_train_split.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tModel Training for Fully Connected Neural Network (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfcnn_best_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m), save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history_fcnn \u001b[38;5;241m=\u001b[39m model_fcnn\u001b[38;5;241m.\u001b[39mfit(X_train_split, y_train_split, \n\u001b[1;32m     21\u001b[0m                                epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m     22\u001b[0m                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m     23\u001b[0m                                validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Use 20% of training data for validation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                                callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, model_checkpoint])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model_fcnn\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# Define FCNN Model Function\n",
    "def create_fcnn(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "    return model\n",
    "\n",
    "# Create FCNN Model\n",
    "model_fcnn = create_fcnn((X_train_split.shape[1],))\n",
    "\n",
    "# Compile model\n",
    "model_fcnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for early stopping & model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(save_path, 'fcnn_best_model.h5'), save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history_fcnn = model_fcnn.fit(X_train_split, y_train_split, \n",
    "                               epochs=50, \n",
    "                               batch_size=32, \n",
    "                               validation_split=0.2,  # Use 20% of training data for validation\n",
    "                               callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_fcnn.evaluate(X_test, y_test)\n",
    "print('FCNN Test Loss: {test_loss}')\n",
    "print('FCNN Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training for Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model_checkpoint_cnn \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_best_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m), save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history_cnn \u001b[38;5;241m=\u001b[39m model_cnn\u001b[38;5;241m.\u001b[39mfit(X_train_split, y_train_split, \n\u001b[1;32m     21\u001b[0m                              epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m     22\u001b[0m                              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m     23\u001b[0m                              validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Use 20% of training data for validation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                              callbacks\u001b[38;5;241m=\u001b[39m[early_stopping_cnn, model_checkpoint_cnn])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m test_loss_cnn, test_accuracy_cnn \u001b[38;5;241m=\u001b[39m model_cnn\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# Define CNN Model Function\n",
    "def create_cnn(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "    return model\n",
    "\n",
    "# Create CNN Model\n",
    "model_cnn = create_cnn((X_train_split.shape[1],))\n",
    "\n",
    "# Compile model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks for early stopping & model checkpoint\n",
    "early_stopping_cnn = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint_cnn = ModelCheckpoint(filepath=os.path.join(save_path, 'cnn_best_model.h5'), save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history_cnn = model_cnn.fit(X_train_split, y_train_split, \n",
    "                             epochs=50, \n",
    "                             batch_size=32, \n",
    "                             validation_split=0.2,  # Use 20% of training data for validation\n",
    "                             callbacks=[early_stopping_cnn, model_checkpoint_cnn])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_cnn, test_accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
    "print('CNN Test Loss: {test_loss_cnn}')\n",
    "print('CNN Test Accuracy: {test_accuracy_cnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance (FCNN & CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For FCNN\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_fcnn \u001b[38;5;241m=\u001b[39m (model_fcnn\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFCNN Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred_fcnn))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# For FCNN\n",
    "y_pred_fcnn = (model_fcnn.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"FCNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_fcnn))\n",
    "\n",
    "# For CNN\n",
    "y_pred_cnn = (model_cnn.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_cnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
